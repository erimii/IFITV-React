import os
import sys
import pandas as pd
import numpy as np
from tqdm import tqdm
from sklearn.metrics.pairwise import cosine_similarity
import torch
from torchvision import models, transforms
from PIL import Image
import requests
from io import BytesIO
from konlpy.tag import Okt
from transformers import AutoTokenizer, AutoModel

from sqlalchemy import create_engine

def load_contents_from_db():
    # engine = create_engine("mysql+pymysql://root:rubi@db:3306/ifitv_db")
    engine = create_engine('mysql+pymysql://root:rubi@localhost:3306/ifitv_db')
    df = pd.read_sql("SELECT * FROM vod_contents", engine)
    df = df[df['thumbnail'].notna() & (df['thumbnail'].str.strip() != '')].reset_index(drop=True)
    return df

# ====================== 1. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ======================
def load_and_clean_metadata(filepath):
    df = pd.read_csv(filepath)
    df = df[df['thumbnail'].notna() & (df['thumbnail'].str.strip() != '')].reset_index(drop=True)
    return df

def load_embeddings(npy_path, df):
    all_embeds = np.load(npy_path)
    assert all_embeds.shape[0] == df.shape[0], "ì„ë² ë”© ë°°ì—´ê³¼ DataFrame ê¸¸ì´ ë¶ˆì¼ì¹˜!"
    return all_embeds

def extract_nouns(text):
    okt = Okt()
    return ' '.join(okt.nouns(str(text)))

def get_bert_embedding(text, tokenizer, model):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()

def build_bert_matrix(desc_list, tokenizer, model):
    embs = []
    for desc in tqdm(desc_list, desc="ì¤„ê±°ë¦¬ KoBERT ì„ë² ë”© ì¶”ì¶œ"):
        try:
            emb = get_bert_embedding(desc, tokenizer, model)
        except Exception:
            emb = np.zeros(768)
        embs.append(emb)
    return np.stack(embs)

# ğŸ”½ 1. KoBERT ì„ë² ë”© ìë™ ìƒì„±
def load_or_build_bert_matrix(df, tokenizer, model, save_path='image_kobert_embedding_matrix.npy'):
    if os.path.exists(save_path):
        print(f"âœ”ï¸ KoBERT ì„ë² ë”© ë¡œë“œ ì¤‘... ({save_path})")
        matrix = np.load(save_path)
    else:
        print("ğŸ“Œ KoBERT ì„ë² ë”© ìƒì„± ì¤‘...")
        desc_list = df['description'].fillna("").tolist()
        matrix = build_bert_matrix(desc_list, tokenizer, model)
        np.save(save_path, matrix)
        print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {save_path}")
    if matrix.shape[0] != df.shape[0]:
        raise ValueError(f"âŒ KoBERT ì„ë² ë”© ê°œìˆ˜ ë¶ˆì¼ì¹˜! {matrix.shape[0]} vs {df.shape[0]}")
    return matrix

# ì¸ë„¤ì¼ ì„ë² ë”© ìë™ ìƒì„±
def load_or_build_image_embeddings(df, save_path='all_embeds.npy'):
    if os.path.exists(save_path):
        print(f"âœ”ï¸ ì´ë¯¸ì§€ ì„ë² ë”© ë¡œë“œ ì¤‘... ({save_path})")
        embeds = np.load(save_path)
    else:
        print("ğŸ“Œ ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± ì¤‘...")
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = models.resnet50(weights='IMAGENET1K_V1')
        model.fc = torch.nn.Identity()
        model = model.to(device)
        model.eval()

        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])
        ])

        embeds = []
        for url in tqdm(df["thumbnail"], desc="ì¸ë„¤ì¼ ì„ë² ë”© ì¤‘"):
            emb = get_image_embedding(url, model, device, transform)
            embeds.append(emb)

        embeds = np.stack(embeds)
        np.save(save_path, embeds)
        print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {save_path}")
    if embeds.shape[0] != df.shape[0]:
        raise ValueError(f"âŒ ì¸ë„¤ì¼ ì„ë² ë”© ê°œìˆ˜ ë¶ˆì¼ì¹˜! {embeds.shape[0]} vs {df.shape[0]}")
    return embeds


# ====================== 2. ì„ë² ë”© ì¶”ì¶œ í•¨ìˆ˜ ======================
def get_image_embedding(url, model, device, transform):
    try:
        response = requests.get(url, timeout=5)
        img = Image.open(BytesIO(response.content)).convert('RGB')
        img_t = transform(img).unsqueeze(0).to(device)
        with torch.no_grad():
            embedding = model(img_t)
            embedding = embedding.cpu().numpy().flatten()
            embedding = embedding / np.linalg.norm(embedding)
        return embedding
    except Exception as e:
        print(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {url} ({e})")
        return np.zeros(2048)

# ====================== 3. ì¶”ì²œ í•¨ìˆ˜ (KoBERT+ì„œë¸Œì¥ë¥´ í¬í•¨) ======================
def recommend_topn(
    df, all_embeds, input_title, bert_matrix, tokenizer, model,
    weights=(0.3, 0.2, 0.2, 0.3), top_n=10
):
    try:
        row = df[df['title'] == input_title].iloc[0]
    except IndexError:
        raise ValueError(f"[ì¶”ì²œ ì‹¤íŒ¨] ì…ë ¥ title '{input_title}'ì´ ë°ì´í„°í”„ë ˆì„ì— ì—†ìŠµë‹ˆë‹¤.")
    your_thumb = row['thumbnail']
    your_genre = row['genre']
    your_subgenres = set([s.strip() for s in str(row['subgenre']).split(',')])
    your_desc = str(row['description']) if pd.notna(row['description']) else ""

    # ëª¨ë¸ ë° ì „ì²˜ë¦¬ ì„¸íŒ…
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    vision_model = models.resnet50(weights='IMAGENET1K_V1')
    vision_model.fc = torch.nn.Identity()
    vision_model = vision_model.to(device)
    vision_model.eval()
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # ì…ë ¥ ì¸ë„¤ì¼ ì„ë² ë”©
    your_embed = get_image_embedding(your_thumb, vision_model, device, transform)
    thumb_sims = cosine_similarity([your_embed], all_embeds)[0]
    df = df.copy()
    df['thumb_sim'] = thumb_sims

    # ì¥ë¥´/ì„œë¸Œì¥ë¥´ ìœ ì‚¬ë„
    def genre_score(x):
        return 1.0 if str(x).strip() == str(your_genre).strip() else 0.0
    def subgenre_score(x):
        if pd.isna(x):
            return 0.0
        row_set = set([s.strip() for s in str(x).split(',')])
        return 1.0 if row_set & your_subgenres else 0.0
    df['genre_sim'] = df['genre'].apply(genre_score)
    df['subgenre_sim'] = df['subgenre'].apply(subgenre_score)

    # KoBERT ì„ë² ë”© ê¸°ë°˜ ì¤„ê±°ë¦¬ ìœ ì‚¬ë„
    your_bert = get_bert_embedding(your_desc, tokenizer, model)
    bert_sim = cosine_similarity([your_bert], bert_matrix)[0]
    df['summary_bert_sim'] = bert_sim

    # ìµœì¢… ê°€ì¤‘ì¹˜ ìœ ì‚¬ë„ (ì¸ë„¤ì¼ 0.3, ì¥ë¥´ 0.2, ì„œë¸Œì¥ë¥´ 0.2, KoBERT 0.3)
    w_thumb, w_genre, w_subg, w_bert = weights
    df['final_score'] = (
        w_thumb * df['thumb_sim'] +
        w_genre * df['genre_sim'] +
        w_subg  * df['subgenre_sim'] +
        w_bert  * df['summary_bert_sim']
    )

    # ìê¸° ìì‹  ì œì™¸ & Top-N ì¶”ì²œ ì¶”ì¶œ
    filtered_df = df[df['title'] != input_title]
    topn = filtered_df.sort_values("final_score", ascending=False).head(top_n)
    return topn

# ====================== 4. ì‹¤í–‰ Entry Point ======================
df = load_contents_from_db()
tokenizer = AutoTokenizer.from_pretrained("monologg/kobert", trust_remote_code=True)
model = AutoModel.from_pretrained("monologg/kobert", trust_remote_code=True)
all_embeds = load_or_build_image_embeddings(df)
bert_matrix = load_or_build_bert_matrix(df, tokenizer, model)

if __name__ == "__main__":
    EMBED_NPY = 'all_embeds.npy'
    BERT_MATRIX_NPY = 'image_kobert_embedding_matrix.npy'
    KOBERT_MODEL = "monologg/kobert"

    input_title = input("ì¶”ì²œë°›ê³  ì‹¶ì€ ì‘í’ˆëª…ì„ ì…ë ¥í•˜ì„¸ìš”: ")
    top_n = 10

    # 1. DBì—ì„œ ì½˜í…ì¸  ë¡œë“œ
    meta_df = load_contents_from_db()
    print(meta_df.shape)

    # 2. KoBERT ëª¨ë¸ ì¤€ë¹„
    tokenizer = AutoTokenizer.from_pretrained(KOBERT_MODEL, trust_remote_code=True)
    model = AutoModel.from_pretrained(KOBERT_MODEL, trust_remote_code=True)

    # 3. ì„ë² ë”© ë¡œë“œ ë˜ëŠ” ìƒì„±
    all_embeds = load_or_build_image_embeddings(meta_df, save_path=EMBED_NPY)
    bert_matrix = load_or_build_bert_matrix(meta_df, tokenizer, model, save_path=BERT_MATRIX_NPY)

    # 4. ì¶”ì²œ ì‹¤í–‰
    result = recommend_topn(
        meta_df,
        all_embeds,
        input_title=input_title,
        bert_matrix=bert_matrix,
        tokenizer=tokenizer,
        model=model,
        weights=(0.3, 0.2, 0.2, 0.3),
        top_n=top_n
    )

    pd.set_option('display.max_columns', None)
    pd.set_option('display.max_rows', None)
    pd.set_option('display.max_colwidth', None)
    pd.set_option('display.width', 2000)
    print(result[[
        'title', 'final_score', 'thumb_sim', 'genre_sim', 'subgenre_sim',
        'summary_bert_sim', 'genre', 'subgenre', 'thumbnail'
    ]])
    print('ì™„ë£Œ!')
